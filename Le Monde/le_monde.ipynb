{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c879d02c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f1627",
   "metadata": {},
   "source": [
    "# Chemins des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ab787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre les chemins des dossiers\n",
    "path_lemonde_diplomatique = 'F:\\Corpus\\Le_Monde_2\\W0036-02'\n",
    "path_lemonde = 'F:\\Corpus\\Le_Monde_2\\W0015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoding(input_file, output_file):\n",
    "    \"\"\"Pour convertir un fichier iso en utf8\"\"\"\n",
    "    with codecs.open(input_file, 'r', encoding='ISO-8859-15') as file:\n",
    "        content = file.read()\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(folder_path):\n",
    "    \"\"\"Extract all files in folder\"\"\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65317cf7",
   "metadata": {},
   "source": [
    "# Extraction de tous les fichiers + garder ceux avec les extensions qui nous intéressent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde diplomatique\n",
    "files = extract_files(path_lemonde_diplomatique)\n",
    "files = [x for x in files if x.endswith('-article.html')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde \n",
    "files2 = extract_files(path_lemonde)\n",
    "files2 = [x for x in files2 if x.endswith('UTF8.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_file(file_path):\n",
    "    try : \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    except UnicodeDecodeError:\n",
    "        print('read_html_file erreur : ', file_path)\n",
    "        soup = None\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_XML_file(file_path):\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'xml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561def8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numb(soup):\n",
    "    all_notes = soup.find_all('p', attrs={'class':'spip_note'})\n",
    "#     numbers = {}\n",
    "#     for spip_note in all_notes: \n",
    "#         a_tag = spip_note.find('a', class_='spip_note')\n",
    "#         if a_tag is not None:\n",
    "#             number = a_tag.get_text().strip()\n",
    "#             numbers['{}'.format(number)] = spip_note.get_text().replace('\\xa0',' ')\n",
    "#     return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(replacement_dict, match):\n",
    "    number = match.group(1)\n",
    "    if number in replacement_dict:\n",
    "        return replacement_dict[number]\n",
    "    return match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spip(soup, text):\n",
    "    new_str = []\n",
    "    replacement_dict = get_numb(soup)\n",
    "    pattern = '\\(([0-9]*)\\)'\n",
    "    for word in text.split():\n",
    "        match = re.match(pattern, word)\n",
    "        if match : \n",
    "            number = match.group(1)\n",
    "            if number in replacement_dict:\n",
    "                replacement = '['+ replacement_dict[number] +']'\n",
    "                word = re.sub(pattern, replacement, word)\n",
    "                new_str.append(word)\n",
    "            else:\n",
    "                new_str.append(word)\n",
    "        else : \n",
    "            new_str.append(word)\n",
    "    #text = re.sub(pattern, replace(replacement_dict, match), text)\n",
    "    #for match in matches:\n",
    "     #   print(match)\n",
    "      #  if match in replacement_dict:\n",
    "       #     replacement = replacement_dict[match]\n",
    "        #    print(replacement)\n",
    "         #   print('---')\n",
    "         #   text = text.replace(f'({match})', replacement)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup, file):\n",
    "    title = soup.find('title').get_text()\n",
    "    authors_element = soup.find('meta', attrs={'name': 'Authors'})\n",
    "    try :\n",
    "        keywords = soup.find('meta', attrs={'name': 'Keywords'}).get('content')\n",
    "    except AttributeError:\n",
    "        keywords = ''\n",
    "    authors = authors_element['content'] if authors_element else None\n",
    "    text = soup.find('span', attrs={'class':'corpsText'}).get_text().replace('\\xa0',' ')#.decode('utf-8')\n",
    "    all_notes = ' '.join([x.get_text().strip() for x in soup.find_all('p', attrs={'class':'spip_note'})])\n",
    "    text = text + all_notes\n",
    "#     text = ' '.join(get_spip(soup, text))\n",
    "    pat = r'F:\\\\Corpus\\\\Le_Monde_2\\\\W0036-02\\\\([^\\\\]*)\\\\([^\\\\]*)\\\\([^\\\\]*)-article\\.html'\n",
    "    match = re.search(pat, file)\n",
    "    name = match.group(1)+'_'+match.group(2)+'_'+match.group(3)\n",
    "    try :\n",
    "        date = soup.find('td', attrs={'class': 'date'}).get_text().replace('\\n','').replace('  ','')\n",
    "    except:\n",
    "        date = match.group(2)+'/'+ match.group(1)\n",
    "    dict_ = {}\n",
    "    dict_['name']=name\n",
    "    dict_['author'] = authors\n",
    "    dict_['mots_clefs'] = keywords\n",
    "    dict_['date'] = date\n",
    "    dict_['text'] = clean(fix_encoding(text)).strip()\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_XML(soup, file):\n",
    "    title = fix_encoding(soup.find('Titre').get_text())\n",
    "    authors = fix_encoding(soup.find('SignaturesIndexees').get_text()).strip()\n",
    "    jour = soup.find('Date').get('Jour')\n",
    "    mois = soup.find('Date').get('Mois')\n",
    "    annee = soup.find('Date').get('Annee')\n",
    "    date = str(jour +'/'+ mois +'/'+ annee)\n",
    "    text = soup.find('Texte').get_text().strip()\n",
    "    text = fix_encoding(text)\n",
    "    \n",
    "    if authors == '':\n",
    "        pat2 = \".*\\. - \\((.*)\\)$\"\n",
    "        match2 = re.search(pat2, text)\n",
    "        if match2:\n",
    "            authors = match2.group(1).replace('.','')\n",
    "            \n",
    "    pat = r'F:\\\\Corpus\\\\Le_Monde_2\\\\W0015\\\\2007\\\\([^\\\\]*)\\\\([^\\\\]*)\\\\([^\\\\]*)\\.xml'\n",
    "    match = re.search(pat, file)\n",
    "    if match :\n",
    "        name = match.group(1)+'_'+match.group(2)+'_'+match.group(3)\n",
    "    else: \n",
    "        pat1 = r'F:\\\\Corpus\\\\Le_Monde_2\\\\W0015\\\\2007\\\\([^\\\\]*)\\\\([^\\\\]*)\\.xml'\n",
    "        match1 = re.search(pat1, file)\n",
    "        name = '0_'+match1.group(1)+'_'+match1.group(2)\n",
    "   \n",
    "    categories = \", \".join([x.get_text() for x in soup.find_all('Categorie')])\n",
    "    \n",
    "    dict_ = {}\n",
    "    dict_['name']=name\n",
    "    dict_['titre']= title\n",
    "    dict_['categories'] = categories\n",
    "    dict_['author'] = authors\n",
    "    dict_['date'] = date\n",
    "    dict_['text'] = clean(fix_encoding(text)).strip()\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(file_path, ext):\n",
    "    if ext ==\"XML\":\n",
    "        soup = read_XML_file(file_path)\n",
    "        if soup:\n",
    "            #dict_ =extract_data(soup, file_path)\n",
    "            dict_ =extract_data_XML(soup, file_path)\n",
    "            return dict_\n",
    "        else:\n",
    "            return None\n",
    "    elif ext ==\"HTML\":\n",
    "        soup = read_html_file(file_path)\n",
    "        if soup:\n",
    "            dict_ =extract_data(soup, file_path)\n",
    "            return dict_\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e931be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub('\\(\\d*\\)', '', text)\n",
    "    text = re.sub('^\\\"', '', text)\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = re.sub('«', '', text)\n",
    "    text = re.sub('»', '', text)\n",
    "    text = re.sub('\\s-\\s\\([^\\.]*\\.\\)$', '', text)\n",
    "    text = re.sub('\\\"', '', text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('\\\"', '')\n",
    "    text = text.replace('“', '')\n",
    "    text = text.replace('”', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('_','')\n",
    "    text = text.replace(' – ', ' ,')\n",
    "    text = text.replace('\\xad','')\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(input_string):\n",
    "    encoding_dict = {\n",
    "        'Ã©': 'é',\n",
    "        'Ãš': 'è',\n",
    "        'Â«': '«',\n",
    "        'Ã\\xa0':'à',\n",
    "        'Ã\\x97':'à',\n",
    "        'Â\\x92':\"\\'\",\n",
    "        'Â»':'»',\n",
    "        'Ã®':'î',\n",
    "        'Ã\\x9b':'Û',\n",
    "        'Ãª':'ê',\n",
    "        'Ã\\x82':'Â',\n",
    "        \"Ã\\xad\": \"í\",\n",
    "        \"Â\\x96\":',',\n",
    "        'Ã¯':'ï',\n",
    "        'Ã¢':'â',\n",
    "        'Ã§':'ç',\n",
    "        'Ã¹':'ù',\n",
    "        \"Ã½\": \"ý\",\n",
    "        \"Ã¼\": \"ü\",\n",
    "        'Ã»':'û',\n",
    "        '\\x9c':'œ',\n",
    "        '\\x8c':'œ',\n",
    "        '\\x93': '',\n",
    "        '\\x94':'',\n",
    "        '\\x1a':'',\n",
    "        'ÃŽ':'ô',\n",
    "        'Â\\x80':' ',\n",
    "        'Ã«':'ë',\n",
    "        'Ã\\x89':'É',\n",
    "        'Ã\\x80':'À',\n",
    "        \"ÃŸ\": \"ß\",\n",
    "        'Ã\\x94':'Ô',\n",
    "        'Â\\x8c':'Œ',\n",
    "        'Ã¶':'ö',\n",
    "        'Ã\\x87':'Ç',\n",
    "        r'\\\\\\'': '\\'',\n",
    "        'Ã\\x88':'È',\n",
    "        'Ã\\x8a':'Ê',\n",
    "        'ÃŒ':'ü',\n",
    "        'Ã\\x8e':'Î',\n",
    "        'Â°':'°',\n",
    "        \"Ã\\x8f\":\"Ï\",\n",
    "        \"Ã±\": \"ñ\",\n",
    "        \"Ã³\": \"ó\",\n",
    "        \"Ã´\": \"ô\",\n",
    "        \"Ã¶\": \"ö\",\n",
    "        \"\\xa0\":\"\",\n",
    "    }\n",
    "\n",
    "    for encoded_char, correct_char in encoding_dict.items():\n",
    "        input_string = input_string.replace(encoded_char, correct_char)\n",
    "\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2article(index):\n",
    "    soup = read_html_file(index)\n",
    "    articles_link = [x.get('href') for x in soup.find_all('a', attrs={'class': 'devtextesommaire'})]\n",
    "    news = [index.replace('index.html','{}'.format(x)).replace('/','\\\\') for x in articles_link]\n",
    "    resume = soup.find('div', attrs={'class':'diplozizi'})\n",
    "    list_dict = []\n",
    "    i = 0\n",
    "    for x in resume.find_all('p', attrs={'class':'spip'}):\n",
    "        dico = {}\n",
    "        i+=1\n",
    "        if i%2!=0:\n",
    "            dico['name']=x.get_text()\n",
    "        elif i%2==0:\n",
    "            dico['text']=x.get_text().replace('&nbsp', ' ')\n",
    "            dico['categorie']='Resumé 1ère page'\n",
    "            dico['date'] = re.sub('F:/Corpus/Le_Monde_2/W0036-02/(\\d*)/(\\d*)/index.html','\\2-\\1',index)\n",
    "            list_dict.append(dico)\n",
    "        \n",
    "    dict1={}\n",
    "    for x in news :\n",
    "        dict_ = {}\n",
    "        soup1 = read_html_file(x)\n",
    "        title = soup1.find('title').get_text()\n",
    "        authors_element = soup1.find('meta', attrs={'name': 'Authors'})\n",
    "        authors = authors_element['content'] if authors_element else None\n",
    "        text = soup1.find('p', attrs={'class':'spip'}).get_text().replace('\\xa0',' ')#.decode('utf-8')\n",
    "        all_notes = ' '.join([x.get_text().strip() for x in soup1.find_all('p', attrs={'class':'spip_note'})])\n",
    "        text = text + all_notes\n",
    "#         text = ' '.join(get_spip(soup1, text))\n",
    "        pat = r'F:\\\\Corpus\\\\Le_Monde_2\\\\W0036-02\\\\(2004)\\\\([^\\\\]*)\\\\([^\\\\]*)\\\\([^\\\\]*)'\n",
    "        match = re.search(pat, x)\n",
    "        name = match.group(1)+'_'+match.group(2)+'_'+match.group(3)+'_'+match.group(4)\n",
    "        date = soup1.find('meta', attrs={'name': 'Date'}).get('content')\n",
    "        rubrique = soup1.find('meta', attrs={'name': 'Rubrique'}).get('content')\n",
    "        try : \n",
    "            themes = soup1.find('meta', attrs={'name': 'Keywords'}).get('content')\n",
    "        except AttributeError:\n",
    "            themes = '' \n",
    "        dict_['name']= name\n",
    "        dict_['categorie']= rubrique\n",
    "        dict_['mots_clefs']=themes\n",
    "        dict_['author'] = authors\n",
    "        dict_['date'] = date\n",
    "        dict_['text'] = clean(fix_encoding(text))\n",
    "        list_dict.append(dict_)\n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0555ec",
   "metadata": {},
   "source": [
    "# Analyse + création d'un df et d'un fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde diplomatique (sauf 2004)\n",
    "list_dict = [create_dict(x,'XML') for x in files2]\n",
    "new = [x for x in list_dict if x]\n",
    "df = pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79908340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "try:\n",
    "    f = codecs.open(r\"F:\\Corpus\\finaux\\Karo\\lemonde2007_3col.tsv\", encoding='utf-8', errors='strict')\n",
    "    for line in f:\n",
    "        pass\n",
    "    print(\"Valid utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"invalid utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('F:\\Corpus\\\\finaux\\\\lemonde2007.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemonde2007= df[['name','author','text']]\n",
    "lemonde2007 = lemonde2007.rename(columns={'name': 'nom', 'author': 'auteur', 'text':'text_clean'})\n",
    "lemonde2007.to_csv(r'F:\\Corpus\\finaux\\Karo\\lemonde2007_3col.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ea6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde diplomatique 2004\n",
    "path2004 = r'F:\\Corpus\\Le_Monde_2\\W0036-02\\2004'\n",
    "files2004 = extract_files(path2004)\n",
    "files2004 = [x for x in files2004 if x.endswith('.html')]\n",
    "listlist = [index2article(x) for x in files2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_data = [item for sublist in listlist for item in sublist]\n",
    "# Convert to dataframe\n",
    "df2004 = pd.DataFrame(flatten_data, columns=['name', 'categorie', 'author', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22822b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2004.to_csv(r'F:\\Corpus\\finaux\\lemonde2004.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2004_2 = df2004[['name','author','text']]\n",
    "df2004_2 = df2004_2.rename(columns={'name': 'nom', 'author': 'auteur', 'text':'text_clean'})\n",
    "df2004_2.to_csv(r'F:\\Corpus\\finaux\\Karo\\lemonde2004_3col.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01040d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde 2007\n",
    "list_dict2 = [create_dict(x,'XML') for x in files2]\n",
    "df2 = pd.DataFrame(list_dict2)\n",
    "df2.to_csv(r'F:\\Corpus\\finaux\\lemonde2007.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour le monde 2012\n",
    "lemonde2012 = 'F:\\\\Corpus\\\\Le_Monde_2\\\\W0015\\\\2012\\\\LeMonde_20120101-20121231.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_line_by_line(file_path):\n",
    "    lines = []\n",
    "    dict1 = {}\n",
    "    i = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        ensemble = []\n",
    "        for line in file:\n",
    "            if line.strip() == '© SA Le Monde - CEDROM-SNi inc. 2013. Tous droits réservés.':\n",
    "                if ensemble:\n",
    "                    dict1[str(i)] = ensemble\n",
    "                    i += 1\n",
    "                    ensemble = []\n",
    "            else:\n",
    "                ensemble.append(line.strip())\n",
    "    \n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60247249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2012 = read_file_line_by_line(lemonde2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_lists(data):\n",
    "    sub_lists = []\n",
    "    sub_list = []\n",
    "    count = 0\n",
    "\n",
    "    for item in data:\n",
    "        if item != '':\n",
    "            sub_list.append(item)\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            if count == 3:\n",
    "                sub_lists.append(sub_list)\n",
    "                sub_list = []\n",
    "                count = 0\n",
    "\n",
    "    # Ajouter la dernière sous-liste\n",
    "    sub_lists.append(sub_list)\n",
    "    return sub_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbee746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_debut_texte(chaine):\n",
    "    mots = chaine.split()\n",
    "    debut_indices = []\n",
    "    mots_cles = ['correspondant', 'correspondance']\n",
    "    mots_cles_composes = ['Envoyé spécial']\n",
    "    \n",
    "    for i, mot in enumerate(mots):\n",
    "        if mot.lower() in mots_cles:\n",
    "            debut_indices.append(i)\n",
    "        elif i < len(mots) - 1:\n",
    "            mot_compose = mot.lower() + ' ' + mots[i + 1].lower()\n",
    "            if mot_compose in mots_cles_composes:\n",
    "                debut_indices.append(i)\n",
    "\n",
    "    if debut_indices:\n",
    "        debut_index = min(debut_indices)\n",
    "        return ' '.join(mots[debut_index + 1:])\n",
    "    return chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    dict_ = {}\n",
    "    sub_lists = get_sub_lists(data)\n",
    "    if check_value_occurrences(sub_lists, sub_lists[3], 'N'):\n",
    "        auteur = sub_lists[3]\n",
    "        next_ = int(check_value_occurrences(sub_lists, sub_lists[3], 'Y'))\n",
    "        text = sub_lists[4:next_]\n",
    "        text = supprimer_debut_texte(' '.join([item for sublist in text for item in sublist]))\n",
    "    else:\n",
    "        auteur = ''\n",
    "        text = ''.join(''.join(sublist) for sublist in sub_lists[3:-3])\n",
    "    pat = r'([0-9]* [a-z-9áàâäãåçéèêëíìîïñóòôöõúùûüýÿæœ]* [0-9]{4}), .*'\n",
    "    pat2 = r'(.*):(.*)'\n",
    "    match = re.search(pat,  sub_lists[1][1])\n",
    "    if len(sub_lists[2])>1:\n",
    "        categorie = ''.join(sub_lists[2][0])\n",
    "        titre = ' '.join(sub_lists[2][1:])\n",
    "    else:\n",
    "        categorie = ''\n",
    "        titre = ' '.join(sub_lists[2])\n",
    "    dict_['journal']= ''.join(sub_lists[1][0])\n",
    "    dict_['date']= match.group(1)\n",
    "    dict_['titre']= titre\n",
    "    dict_['categorie'] = categorie\n",
    "    dict_['auteur'] = ' '.join(auteur)\n",
    "    dict_['texte'] = clean(fix_encoding(text)).replace('\\n', '').strip()\n",
    "    \n",
    "    for sous_liste in sub_lists[-2]:\n",
    "        match2 = re.search(pat2, sous_liste)\n",
    "        dict_['{}'.format(match2.group(1).strip())] = match2.group(2).strip()\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_occurrences(lst_of_lists, value, response):\n",
    "    count = 0\n",
    "    index = None\n",
    "    for i, sublist in enumerate(lst_of_lists):\n",
    "        if sublist == value:\n",
    "            count += 1\n",
    "            if count == 2:\n",
    "                index = i\n",
    "                break\n",
    "    if response == 'Y':\n",
    "        return index\n",
    "    else:\n",
    "        return count == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict2012 = [extract(v) for k, v in dict2012.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012 = pd.DataFrame(list_dict2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012 = df2012[['journal', 'date', 'titre', 'categorie', 'auteur', 'texte', 'Section','Taille', 'Type d\\'article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012.to_csv(r'F:\\Corpus\\finaux\\lemonde2012.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c33611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012_2 = df2012[['titre','auteur','texte']]\n",
    "df2012_2 = df2012_2.rename(columns={'titre': 'nom', 'texte':'text_clean'})\n",
    "df2012_2.to_csv(r'F:\\Corpus\\finaux\\Karo\\lemonde2012_3col.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
